{"version":3,"file":"415.js","mappings":"2UA+C4C,I,0CC/BN,I,yHCV/B,MAAMA,UAA4B,KASrC,qBAAAC,CAAsBC,EAAaC,EAASC,GACxC,OAAOC,KAAKC,YAAYJ,EAAaE,EACzC,CACA,oBAAAG,CAAqBC,GACjB,MAAkC,iBAApBA,EAAQC,QAChBD,EAAQC,QACRJ,KAAKK,4BAA4BF,EAAQC,QACnD,CACA,2BAAAC,CAA4BD,GACxB,OAAOE,KAAKC,UAAUH,EAC1B,CAWA,YAAMI,CAAOC,EAAOC,GAChB,MAAqB,iBAAVD,EACAT,KAAKW,iBAAgBC,MAAOH,EAAOC,IAAYV,KAAKC,YAAY,CAAC,CAAEY,KAAMJ,IAAUC,GAASX,YAAYU,EAAO,IAAKC,EAASI,QAAS,WAGtId,KAAKW,iBAAgBC,MAAOH,EAAOC,IAAYV,KAAKC,YAAY,CACnE,CACIE,QAASM,EACTI,KAAMb,KAAKE,qBAAqBO,KAErCC,GAASX,YAAYU,EAAO,IAAKC,EAASI,QAAS,UAE9D,EAKG,MAAMC,UAAyBpB,EAClC,WAAAM,CAAYJ,EAAaE,GACrB,OAAOC,KAAKgB,MAAMnB,EAAY,GAAGgB,KAAMd,EAC3C,CACA,qBAAMkB,CAAgBJ,EAAMf,EAASC,GACjC,OAAOC,KAAKgB,MAAMH,EAAMd,EAC5B,CAIA,KAAAmB,GACI,MAAM,IAAIC,MAAM,wBACpB,EAoBuCA,M,2BCtB3B,oBAATC,MACHA,KAAKC,UACoB,SAAzBD,KAAKC,SAASC,SAEcF,KAAKC,SAASC,OAASF,KAAKC,SAASE,SAAWF,SAASG,Q,oCC3DlF,MAAMC,UAAyBV,EAClC,WAAAW,GACIC,SAASC,WACTC,OAAOC,eAAe9B,KAAM,eAAgB,CACxC+B,YAAY,EACZC,cAAc,EACdC,UAAU,EACVC,MAAO,CAAC,YAAa,iBAAkB,aAE3CL,OAAOC,eAAe9B,KAAM,kBAAmB,CAC3C+B,YAAY,EACZC,cAAc,EACdC,UAAU,EACVC,OAAO,GAEf,CACA,cAAOC,GACH,MAAO,kBACX,CAOA,KAAAnB,CAAMH,GACF,OAAOuB,QAAQC,QAAQxB,EAC3B,CAMA,qBAAAyB,GACI,MAAO,EACX,ECnCJ,SAASC,EAAkBC,GACvB,GAJJ,SAA6BA,GACzB,MAAmC,mBAArBA,EAAQC,QAC1B,CAEQC,CAAoBF,GACpB,OAAOA,EAEN,GAAI,UAAWA,GAAW,KAASG,WAAWH,EAAQI,OACvD,OAAOL,EAAkBC,EAAQI,OAEhC,GAAI,aAAcJ,GACnB,cAAeA,GACf,KAASG,WAAWH,EAAQK,UAC5B,OAAON,EAAkBC,EAAQK,UAEhC,GAAI,YAAaL,GAAW,KAASG,WAAWH,EAAQM,SACzD,OAAOP,EAAkBC,EAAQM,SAGjC,MAAM,IAAI3B,MAAM,2DAExB,CAcO,MAAM4B,UAAiB,IAC1B,cAAOZ,GACH,MAAO,UACX,CACA,aAAIa,GACA,OAAOhD,KAAKiD,OAAOC,cACvB,CACA,cAAIC,GACA,MAAO,CAACnD,KAAKoD,UACjB,CACA,WAAA1B,CAAY2B,GA4CR,GA3CA1B,MAAM0B,GACNxB,OAAOC,eAAe9B,KAAM,kBAAmB,CAC3C+B,YAAY,EACZC,cAAc,EACdC,UAAU,EACVC,OAAO,IAEXL,OAAOC,eAAe9B,KAAM,SAAU,CAClC+B,YAAY,EACZC,cAAc,EACdC,UAAU,EACVC,WAAO,IAEXL,OAAOC,eAAe9B,KAAM,MAAO,CAC/B+B,YAAY,EACZC,cAAc,EACdC,UAAU,EACVC,WAAO,IAEXL,OAAOC,eAAe9B,KAAM,YAAa,CACrC+B,YAAY,EACZC,cAAc,EACdC,UAAU,EACVC,WAAO,IAEXL,OAAOC,eAAe9B,KAAM,YAAa,CACrC+B,YAAY,EACZC,cAAc,EACdC,UAAU,EACVC,MAAO,SAEXL,OAAOC,eAAe9B,KAAM,eAAgB,CACxC+B,YAAY,EACZC,cAAc,EACdC,UAAU,EACVC,WAAO,IAEXlC,KAAKiD,OAASI,EAAOJ,OACrBjD,KAAKsD,IAAMD,EAAOC,IAClBtD,KAAKuD,UAAYF,EAAOE,UACxBvD,KAAKoD,UAAYC,EAAOD,WAAapD,KAAKoD,UAC1CpD,KAAKwD,aACDH,EAAOG,cAAgB,IAAI/B,EAC3BzB,KAAKiD,OAAOO,aAAc,CAC1B,GAAIH,EAAOG,aACP,MAAM,IAAIrC,MAAM,wDAEpBnB,KAAKwD,aAAexD,KAAKiD,OAAOO,YACpC,CACJ,CACA,WAAAC,GAEI,MADiB,aAAczD,KAAKsD,IAAMtD,KAAKsD,IAAII,SAAW,EAElE,CAEA,mBAAAC,CAAoBC,GAChB,MAAMC,EAAkBlC,MAAMgC,oBAAoBC,GAC5CF,EAAW1D,KAAKyD,cACtB,IAAK,MAAMK,KAAOJ,EACVI,KAAOF,UACAC,EAAgBC,GAG/B,OAAOD,CACX,CAEA,qBAAME,CAAgBlE,EAAamE,EAAaC,GAC5C,IAAIC,EAOJ,OALIA,EADAlE,KAAKwD,mBACmBxD,KAAKwD,aAAa5D,sBAAsBC,EAAamE,EAAaC,GAAYE,YAGpFtE,EAAY,GAAGgB,KAE9BqD,CACX,CAMA,IAAAE,CAAKR,EAAQS,GACT,OAAO1C,MAAMyC,KAAKR,EAAQS,EAC9B,CAEA,WAAMC,CAAMV,EAAQK,GAChB,MAAMM,EAAkB,IAAKX,GACvBY,EAAe,IACdxE,KAAKuD,WAENG,EAAW1D,KAAKyD,cACtB,IAAK,MAAMK,KAAOJ,EACVI,KAAOF,GACHY,IACAA,EAAaV,GACTF,EAAOE,UACJS,EAAgBT,IAInC,MAAME,QAAoBhE,KAAKiD,OAAOwB,kBAAkBF,GACxD,GAAI,mBAAoBvE,KAAKsD,IAAK,CAC9B,MAAM,YAAEzD,SAAsBG,KAAKsD,IAAIoB,eAAe,CAACV,GAAcQ,EAAcP,GAAYE,YAC/F,MAAO,CACH,CAACnE,KAAKoD,iBAAkBpD,KAAK+D,gBAAgBlE,EAAY,GAAImE,EAAaC,GAElF,CACA,MAAMU,EAAkB3E,KAAKwD,aACvBxD,KAAKsD,IAAIsB,KAAK5E,KAAKwD,cACnBxD,KAAKsD,IACLuB,QAAiBF,EAAgBnE,OAAOwD,EAAaC,GAAYE,YACvE,MAAO,CACH,CAACnE,KAAKoD,WAAYyB,EAE1B,CAaA,aAAMC,CAAQlB,EAAQmB,GAElB,aADqB/E,KAAKoE,KAAKR,EAAQmB,IACzB/E,KAAKoD,UACvB,CACA,UAAA4B,GACI,MAAO,KACX,CACA,wBAAaC,CAAYC,GACrB,MAAM,IAAE5B,EAAG,OAAEL,GAAWiC,EACxB,IAAK5B,EACD,MAAM,IAAInC,MAAM,0BAEpB,IAAK8B,EACD,MAAM,IAAI9B,MAAM,6BAEpB,OAAO,IAAI4B,EAAS,CAChBO,UAAW,KAAkB2B,YAAY3B,GACzCL,aAAc,KAAmBgC,YAAYhC,IAErD,CAEA,SAAAkC,GACI,MAAMA,EAAY,cAAenF,KAAKsD,IAAMtD,KAAKsD,IAAI6B,iBAAcC,EACnE,MAAO,CACHlE,MAAO,GAAGlB,KAAKgF,qBACf1B,IAAK6B,EACLlC,OAAQjD,KAAKiD,OAAOkC,YAE5B,CACA,aAAAE,CAAcxE,GACV,OAAO0B,EAAkBvC,KAAKsD,KAAKgC,aAAazE,EACpD,E","sources":["webpack://inqviz-excel/./node_modules/@langchain/core/dist/prompts/pipeline.js","webpack://inqviz-excel/./node_modules/@langchain/core/dist/prompts/structured.js","webpack://inqviz-excel/./node_modules/@langchain/core/dist/output_parsers/base.js","webpack://inqviz-excel/./node_modules/@langchain/core/dist/utils/@cfworker/json-schema/src/dereference.js","webpack://inqviz-excel/./node_modules/langchain/dist/output_parsers/noop.js","webpack://inqviz-excel/./node_modules/langchain/dist/chains/llm_chain.js"],"sourcesContent":["import { BasePromptTemplate } from \"./base.js\";\nimport { ChatPromptTemplate } from \"./chat.js\";\n/**\n * Class that handles a sequence of prompts, each of which may require\n * different input variables. Includes methods for formatting these\n * prompts, extracting required input values, and handling partial\n * prompts.\n * @example\n * ```typescript\n * const composedPrompt = new PipelinePromptTemplate({\n *   pipelinePrompts: [\n *     {\n *       name: \"introduction\",\n *       prompt: PromptTemplate.fromTemplate(`You are impersonating {person}.`),\n *     },\n *     {\n *       name: \"example\",\n *       prompt: PromptTemplate.fromTemplate(\n *         `Here's an example of an interaction:\n * Q: {example_q}\n * A: {example_a}`,\n *       ),\n *     },\n *     {\n *       name: \"start\",\n *       prompt: PromptTemplate.fromTemplate(\n *         `Now, do this for real!\n * Q: {input}\n * A:`,\n *       ),\n *     },\n *   ],\n *   finalPrompt: PromptTemplate.fromTemplate(\n *     `{introduction}\n * {example}\n * {start}`,\n *   ),\n * });\n *\n * const formattedPrompt = await composedPrompt.format({\n *   person: \"Elon Musk\",\n *   example_q: `What's your favorite car?`,\n *   example_a: \"Tesla\",\n *   input: `What's your favorite social media site?`,\n * });\n * ```\n */\nexport class PipelinePromptTemplate extends BasePromptTemplate {\n    static lc_name() {\n        return \"PipelinePromptTemplate\";\n    }\n    constructor(input) {\n        super({ ...input, inputVariables: [] });\n        Object.defineProperty(this, \"pipelinePrompts\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"finalPrompt\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.pipelinePrompts = input.pipelinePrompts;\n        this.finalPrompt = input.finalPrompt;\n        this.inputVariables = this.computeInputValues();\n    }\n    /**\n     * Computes the input values required by the pipeline prompts.\n     * @returns Array of input values required by the pipeline prompts.\n     */\n    computeInputValues() {\n        const intermediateValues = this.pipelinePrompts.map((pipelinePrompt) => pipelinePrompt.name);\n        const inputValues = this.pipelinePrompts\n            .map((pipelinePrompt) => pipelinePrompt.prompt.inputVariables.filter((inputValue) => !intermediateValues.includes(inputValue)))\n            .flat();\n        return [...new Set(inputValues)];\n    }\n    static extractRequiredInputValues(allValues, requiredValueNames) {\n        return requiredValueNames.reduce((requiredValues, valueName) => {\n            // eslint-disable-next-line no-param-reassign\n            requiredValues[valueName] = allValues[valueName];\n            return requiredValues;\n        }, {});\n    }\n    /**\n     * Formats the pipeline prompts based on the provided input values.\n     * @param values Input values to format the pipeline prompts.\n     * @returns Promise that resolves with the formatted input values.\n     */\n    async formatPipelinePrompts(values) {\n        const allValues = await this.mergePartialAndUserVariables(values);\n        for (const { name: pipelinePromptName, prompt: pipelinePrompt } of this\n            .pipelinePrompts) {\n            const pipelinePromptInputValues = PipelinePromptTemplate.extractRequiredInputValues(allValues, pipelinePrompt.inputVariables);\n            // eslint-disable-next-line no-instanceof/no-instanceof\n            if (pipelinePrompt instanceof ChatPromptTemplate) {\n                allValues[pipelinePromptName] = await pipelinePrompt.formatMessages(pipelinePromptInputValues);\n            }\n            else {\n                allValues[pipelinePromptName] = await pipelinePrompt.format(pipelinePromptInputValues);\n            }\n        }\n        return PipelinePromptTemplate.extractRequiredInputValues(allValues, this.finalPrompt.inputVariables);\n    }\n    /**\n     * Formats the final prompt value based on the provided input values.\n     * @param values Input values to format the final prompt value.\n     * @returns Promise that resolves with the formatted final prompt value.\n     */\n    async formatPromptValue(values) {\n        return this.finalPrompt.formatPromptValue(await this.formatPipelinePrompts(values));\n    }\n    async format(values) {\n        return this.finalPrompt.format(await this.formatPipelinePrompts(values));\n    }\n    /**\n     * Handles partial prompts, which are prompts that have been partially\n     * filled with input values.\n     * @param values Partial input values.\n     * @returns Promise that resolves with a new PipelinePromptTemplate instance with updated input variables.\n     */\n    async partial(values) {\n        const promptDict = { ...this };\n        promptDict.inputVariables = this.inputVariables.filter((iv) => !(iv in values));\n        promptDict.partialVariables = {\n            ...(this.partialVariables ?? {}),\n            ...values,\n        };\n        return new PipelinePromptTemplate(promptDict);\n    }\n    serialize() {\n        throw new Error(\"Not implemented.\");\n    }\n    _getPromptType() {\n        return \"pipeline\";\n    }\n}\n","import { ChatPromptTemplate, } from \"./chat.js\";\nfunction isWithStructuredOutput(x\n// eslint-disable-next-line @typescript-eslint/ban-types\n) {\n    return (typeof x === \"object\" &&\n        x != null &&\n        \"withStructuredOutput\" in x &&\n        typeof x.withStructuredOutput === \"function\");\n}\nfunction isRunnableBinding(x) {\n    return (typeof x === \"object\" &&\n        x != null &&\n        \"lc_id\" in x &&\n        Array.isArray(x.lc_id) &&\n        x.lc_id.join(\"/\") === \"langchain_core/runnables/RunnableBinding\");\n}\nexport class StructuredPrompt extends ChatPromptTemplate {\n    get lc_aliases() {\n        return {\n            ...super.lc_aliases,\n            schema: \"schema_\",\n        };\n    }\n    constructor(input) {\n        super(input);\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\n        Object.defineProperty(this, \"schema\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"lc_namespace\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: [\"langchain_core\", \"prompts\", \"structured\"]\n        });\n        this.schema = input.schema;\n    }\n    pipe(coerceable) {\n        if (isWithStructuredOutput(coerceable)) {\n            return super.pipe(coerceable.withStructuredOutput(this.schema));\n        }\n        if (isRunnableBinding(coerceable) &&\n            isWithStructuredOutput(coerceable.bound)) {\n            return super.pipe(coerceable.bound\n                .withStructuredOutput(this.schema)\n                .bind(coerceable.kwargs ?? {})\n                .withConfig(coerceable.config));\n        }\n        throw new Error(`Structured prompts need to be piped to a language model that supports the \"withStructuredOutput()\" method.`);\n    }\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    static fromMessagesAndSchema(promptMessages, schema\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    ) {\n        return StructuredPrompt.fromMessages(promptMessages, { schema });\n    }\n}\n","import { Runnable } from \"../runnables/index.js\";\n/**\n * Abstract base class for parsing the output of a Large Language Model\n * (LLM) call. It provides methods for parsing the result of an LLM call\n * and invoking the parser with a given input.\n */\nexport class BaseLLMOutputParser extends Runnable {\n    /**\n     * Parses the result of an LLM call with a given prompt. By default, it\n     * simply calls `parseResult`.\n     * @param generations The generations from an LLM call.\n     * @param _prompt The prompt used in the LLM call.\n     * @param callbacks Optional callbacks.\n     * @returns A promise of the parsed output.\n     */\n    parseResultWithPrompt(generations, _prompt, callbacks) {\n        return this.parseResult(generations, callbacks);\n    }\n    _baseMessageToString(message) {\n        return typeof message.content === \"string\"\n            ? message.content\n            : this._baseMessageContentToString(message.content);\n    }\n    _baseMessageContentToString(content) {\n        return JSON.stringify(content);\n    }\n    /**\n     * Calls the parser with a given input and optional configuration options.\n     * If the input is a string, it creates a generation with the input as\n     * text and calls `parseResult`. If the input is a `BaseMessage`, it\n     * creates a generation with the input as a message and the content of the\n     * input as text, and then calls `parseResult`.\n     * @param input The input to the parser, which can be a string or a `BaseMessage`.\n     * @param options Optional configuration options.\n     * @returns A promise of the parsed output.\n     */\n    async invoke(input, options) {\n        if (typeof input === \"string\") {\n            return this._callWithConfig(async (input, options) => this.parseResult([{ text: input }], options?.callbacks), input, { ...options, runType: \"parser\" });\n        }\n        else {\n            return this._callWithConfig(async (input, options) => this.parseResult([\n                {\n                    message: input,\n                    text: this._baseMessageToString(input),\n                },\n            ], options?.callbacks), input, { ...options, runType: \"parser\" });\n        }\n    }\n}\n/**\n * Class to parse the output of an LLM call.\n */\nexport class BaseOutputParser extends BaseLLMOutputParser {\n    parseResult(generations, callbacks) {\n        return this.parse(generations[0].text, callbacks);\n    }\n    async parseWithPrompt(text, _prompt, callbacks) {\n        return this.parse(text, callbacks);\n    }\n    /**\n     * Return the string type key uniquely identifying this class of parser\n     */\n    _type() {\n        throw new Error(\"_type not implemented\");\n    }\n}\n/**\n * Exception that output parsers should raise to signify a parsing error.\n *\n * This exists to differentiate parsing errors from other code or execution errors\n * that also may arise inside the output parser. OutputParserExceptions will be\n * available to catch and handle in ways to fix the parsing error, while other\n * errors will be raised.\n *\n * @param message - The error that's being re-raised or an error message.\n * @param llmOutput - String model output which is error-ing.\n * @param observation - String explanation of error which can be passed to a\n *     model to try and remediate the issue.\n * @param sendToLLM - Whether to send the observation and llm_output back to an Agent\n *     after an OutputParserException has been raised. This gives the underlying\n *     model driving the agent the context that the previous output was improperly\n *     structured, in the hopes that it will update the output to the correct\n *     format.\n */\nexport class OutputParserException extends Error {\n    constructor(message, llmOutput, observation, sendToLLM = false) {\n        super(message);\n        Object.defineProperty(this, \"llmOutput\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"observation\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"sendToLLM\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.llmOutput = llmOutput;\n        this.observation = observation;\n        this.sendToLLM = sendToLLM;\n        if (sendToLLM) {\n            if (observation === undefined || llmOutput === undefined) {\n                throw new Error(\"Arguments 'observation' & 'llmOutput' are required if 'sendToLlm' is true\");\n            }\n        }\n    }\n}\n","import { encodePointer } from \"./pointer.js\";\nexport const schemaKeyword = {\n    additionalItems: true,\n    unevaluatedItems: true,\n    items: true,\n    contains: true,\n    additionalProperties: true,\n    unevaluatedProperties: true,\n    propertyNames: true,\n    not: true,\n    if: true,\n    then: true,\n    else: true,\n};\nexport const schemaArrayKeyword = {\n    prefixItems: true,\n    items: true,\n    allOf: true,\n    anyOf: true,\n    oneOf: true,\n};\nexport const schemaMapKeyword = {\n    $defs: true,\n    definitions: true,\n    properties: true,\n    patternProperties: true,\n    dependentSchemas: true,\n};\nexport const ignoredKeyword = {\n    id: true,\n    $id: true,\n    $ref: true,\n    $schema: true,\n    $anchor: true,\n    $vocabulary: true,\n    $comment: true,\n    default: true,\n    enum: true,\n    const: true,\n    required: true,\n    type: true,\n    maximum: true,\n    minimum: true,\n    exclusiveMaximum: true,\n    exclusiveMinimum: true,\n    multipleOf: true,\n    maxLength: true,\n    minLength: true,\n    pattern: true,\n    format: true,\n    maxItems: true,\n    minItems: true,\n    uniqueItems: true,\n    maxProperties: true,\n    minProperties: true,\n};\n/**\n * Default base URI for schemas without an $id.\n * https://json-schema.org/draft/2019-09/json-schema-core.html#initial-base\n * https://tools.ietf.org/html/rfc3986#section-5.1\n */\nexport let initialBaseURI = \n// @ts-ignore\ntypeof self !== \"undefined\" &&\n    self.location &&\n    self.location.origin !== \"null\"\n    ? //@ts-ignore\n        /* #__PURE__ */ new URL(self.location.origin + self.location.pathname + location.search)\n    : /* #__PURE__ */ new URL(\"https://github.com/cfworker\");\nexport function dereference(schema, lookup = Object.create(null), baseURI = initialBaseURI, basePointer = \"\") {\n    if (schema && typeof schema === \"object\" && !Array.isArray(schema)) {\n        const id = schema.$id || schema.id;\n        if (id) {\n            const url = new URL(id, baseURI.href);\n            if (url.hash.length > 1) {\n                lookup[url.href] = schema;\n            }\n            else {\n                url.hash = \"\"; // normalize hash https://url.spec.whatwg.org/#dom-url-hash\n                if (basePointer === \"\") {\n                    baseURI = url;\n                }\n                else {\n                    dereference(schema, lookup, baseURI);\n                }\n            }\n        }\n    }\n    else if (schema !== true && schema !== false) {\n        return lookup;\n    }\n    // compute the schema's URI and add it to the mapping.\n    const schemaURI = baseURI.href + (basePointer ? \"#\" + basePointer : \"\");\n    if (lookup[schemaURI] !== undefined) {\n        throw new Error(`Duplicate schema URI \"${schemaURI}\".`);\n    }\n    lookup[schemaURI] = schema;\n    // exit early if this is a boolean schema.\n    if (schema === true || schema === false) {\n        return lookup;\n    }\n    // set the schema's absolute URI.\n    if (schema.__absolute_uri__ === undefined) {\n        Object.defineProperty(schema, \"__absolute_uri__\", {\n            enumerable: false,\n            value: schemaURI,\n        });\n    }\n    // if a $ref is found, resolve it's absolute URI.\n    if (schema.$ref && schema.__absolute_ref__ === undefined) {\n        const url = new URL(schema.$ref, baseURI.href);\n        url.hash = url.hash; // normalize hash https://url.spec.whatwg.org/#dom-url-hash\n        Object.defineProperty(schema, \"__absolute_ref__\", {\n            enumerable: false,\n            value: url.href,\n        });\n    }\n    // if a $recursiveRef is found, resolve it's absolute URI.\n    if (schema.$recursiveRef && schema.__absolute_recursive_ref__ === undefined) {\n        const url = new URL(schema.$recursiveRef, baseURI.href);\n        url.hash = url.hash; // normalize hash https://url.spec.whatwg.org/#dom-url-hash\n        Object.defineProperty(schema, \"__absolute_recursive_ref__\", {\n            enumerable: false,\n            value: url.href,\n        });\n    }\n    // if an $anchor is found, compute it's URI and add it to the mapping.\n    if (schema.$anchor) {\n        const url = new URL(\"#\" + schema.$anchor, baseURI.href);\n        lookup[url.href] = schema;\n    }\n    // process subschemas.\n    for (let key in schema) {\n        if (ignoredKeyword[key]) {\n            continue;\n        }\n        const keyBase = `${basePointer}/${encodePointer(key)}`;\n        const subSchema = schema[key];\n        if (Array.isArray(subSchema)) {\n            if (schemaArrayKeyword[key]) {\n                const length = subSchema.length;\n                for (let i = 0; i < length; i++) {\n                    dereference(subSchema[i], lookup, baseURI, `${keyBase}/${i}`);\n                }\n            }\n        }\n        else if (schemaMapKeyword[key]) {\n            for (let subKey in subSchema) {\n                dereference(subSchema[subKey], lookup, baseURI, `${keyBase}/${encodePointer(subKey)}`);\n            }\n        }\n        else {\n            dereference(subSchema, lookup, baseURI, keyBase);\n        }\n    }\n    return lookup;\n}\n// schema identification examples\n// https://json-schema.org/draft/2019-09/json-schema-core.html#rfc.appendix.A\n// $ref delegation\n// https://github.com/json-schema-org/json-schema-spec/issues/514\n// output format\n// https://json-schema.org/draft/2019-09/json-schema-core.html#output\n// JSON pointer\n// https://tools.ietf.org/html/rfc6901\n// JSON relative pointer\n// https://tools.ietf.org/html/draft-handrews-relative-json-pointer-01\n","import { BaseOutputParser } from \"@langchain/core/output_parsers\";\n/**\n * The NoOpOutputParser class is a type of output parser that does not\n * perform any operations on the output. It extends the BaseOutputParser\n * class and is part of the LangChain's output parsers module. This class\n * is useful in scenarios where the raw output of the Large Language\n * Models (LLMs) is required.\n */\nexport class NoOpOutputParser extends BaseOutputParser {\n    constructor() {\n        super(...arguments);\n        Object.defineProperty(this, \"lc_namespace\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: [\"langchain\", \"output_parsers\", \"default\"]\n        });\n        Object.defineProperty(this, \"lc_serializable\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: true\n        });\n    }\n    static lc_name() {\n        return \"NoOpOutputParser\";\n    }\n    /**\n     * This method takes a string as input and returns the same string as\n     * output. It does not perform any operations on the input string.\n     * @param text The input string to be parsed.\n     * @returns The same input string without any operations performed on it.\n     */\n    parse(text) {\n        return Promise.resolve(text);\n    }\n    /**\n     * This method returns an empty string. It does not provide any formatting\n     * instructions.\n     * @returns An empty string, indicating no formatting instructions.\n     */\n    getFormatInstructions() {\n        return \"\";\n    }\n}\n","import { BaseLanguageModel, } from \"@langchain/core/language_models/base\";\nimport { BasePromptTemplate } from \"@langchain/core/prompts\";\nimport { Runnable } from \"@langchain/core/runnables\";\nimport { BaseChain } from \"./base.js\";\nimport { NoOpOutputParser } from \"../output_parsers/noop.js\";\nfunction isBaseLanguageModel(llmLike) {\n    return typeof llmLike._llmType === \"function\";\n}\nfunction _getLanguageModel(llmLike) {\n    if (isBaseLanguageModel(llmLike)) {\n        return llmLike;\n    }\n    else if (\"bound\" in llmLike && Runnable.isRunnable(llmLike.bound)) {\n        return _getLanguageModel(llmLike.bound);\n    }\n    else if (\"runnable\" in llmLike &&\n        \"fallbacks\" in llmLike &&\n        Runnable.isRunnable(llmLike.runnable)) {\n        return _getLanguageModel(llmLike.runnable);\n    }\n    else if (\"default\" in llmLike && Runnable.isRunnable(llmLike.default)) {\n        return _getLanguageModel(llmLike.default);\n    }\n    else {\n        throw new Error(\"Unable to extract BaseLanguageModel from llmLike object.\");\n    }\n}\n/**\n * Chain to run queries against LLMs.\n *\n * @example\n * ```ts\n * import { LLMChain } from \"langchain/chains\";\n * import { OpenAI } from \"langchain/llms/openai\";\n * import { PromptTemplate } from \"langchain/prompts\";\n *\n * const prompt = PromptTemplate.fromTemplate(\"Tell me a {adjective} joke\");\n * const llm = new LLMChain({ llm: new OpenAI(), prompt });\n * ```\n */\nexport class LLMChain extends BaseChain {\n    static lc_name() {\n        return \"LLMChain\";\n    }\n    get inputKeys() {\n        return this.prompt.inputVariables;\n    }\n    get outputKeys() {\n        return [this.outputKey];\n    }\n    constructor(fields) {\n        super(fields);\n        Object.defineProperty(this, \"lc_serializable\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: true\n        });\n        Object.defineProperty(this, \"prompt\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"llm\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"llmKwargs\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"outputKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"text\"\n        });\n        Object.defineProperty(this, \"outputParser\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.prompt = fields.prompt;\n        this.llm = fields.llm;\n        this.llmKwargs = fields.llmKwargs;\n        this.outputKey = fields.outputKey ?? this.outputKey;\n        this.outputParser =\n            fields.outputParser ?? new NoOpOutputParser();\n        if (this.prompt.outputParser) {\n            if (fields.outputParser) {\n                throw new Error(\"Cannot set both outputParser and prompt.outputParser\");\n            }\n            this.outputParser = this.prompt.outputParser;\n        }\n    }\n    getCallKeys() {\n        const callKeys = \"callKeys\" in this.llm ? this.llm.callKeys : [];\n        return callKeys;\n    }\n    /** @ignore */\n    _selectMemoryInputs(values) {\n        const valuesForMemory = super._selectMemoryInputs(values);\n        const callKeys = this.getCallKeys();\n        for (const key of callKeys) {\n            if (key in values) {\n                delete valuesForMemory[key];\n            }\n        }\n        return valuesForMemory;\n    }\n    /** @ignore */\n    async _getFinalOutput(generations, promptValue, runManager) {\n        let finalCompletion;\n        if (this.outputParser) {\n            finalCompletion = await this.outputParser.parseResultWithPrompt(generations, promptValue, runManager?.getChild());\n        }\n        else {\n            finalCompletion = generations[0].text;\n        }\n        return finalCompletion;\n    }\n    /**\n     * Run the core logic of this chain and add to output if desired.\n     *\n     * Wraps _call and handles memory.\n     */\n    call(values, config) {\n        return super.call(values, config);\n    }\n    /** @ignore */\n    async _call(values, runManager) {\n        const valuesForPrompt = { ...values };\n        const valuesForLLM = {\n            ...this.llmKwargs,\n        };\n        const callKeys = this.getCallKeys();\n        for (const key of callKeys) {\n            if (key in values) {\n                if (valuesForLLM) {\n                    valuesForLLM[key] =\n                        values[key];\n                    delete valuesForPrompt[key];\n                }\n            }\n        }\n        const promptValue = await this.prompt.formatPromptValue(valuesForPrompt);\n        if (\"generatePrompt\" in this.llm) {\n            const { generations } = await this.llm.generatePrompt([promptValue], valuesForLLM, runManager?.getChild());\n            return {\n                [this.outputKey]: await this._getFinalOutput(generations[0], promptValue, runManager),\n            };\n        }\n        const modelWithParser = this.outputParser\n            ? this.llm.pipe(this.outputParser)\n            : this.llm;\n        const response = await modelWithParser.invoke(promptValue, runManager?.getChild());\n        return {\n            [this.outputKey]: response,\n        };\n    }\n    /**\n     * Format prompt with values and pass to LLM\n     *\n     * @param values - keys to pass to prompt template\n     * @param callbackManager - CallbackManager to use\n     * @returns Completion from LLM.\n     *\n     * @example\n     * ```ts\n     * llm.predict({ adjective: \"funny\" })\n     * ```\n     */\n    async predict(values, callbackManager) {\n        const output = await this.call(values, callbackManager);\n        return output[this.outputKey];\n    }\n    _chainType() {\n        return \"llm\";\n    }\n    static async deserialize(data) {\n        const { llm, prompt } = data;\n        if (!llm) {\n            throw new Error(\"LLMChain must have llm\");\n        }\n        if (!prompt) {\n            throw new Error(\"LLMChain must have prompt\");\n        }\n        return new LLMChain({\n            llm: await BaseLanguageModel.deserialize(llm),\n            prompt: await BasePromptTemplate.deserialize(prompt),\n        });\n    }\n    /** @deprecated */\n    serialize() {\n        const serialize = \"serialize\" in this.llm ? this.llm.serialize() : undefined;\n        return {\n            _type: `${this._chainType()}_chain`,\n            llm: serialize,\n            prompt: this.prompt.serialize(),\n        };\n    }\n    _getNumTokens(text) {\n        return _getLanguageModel(this.llm).getNumTokens(text);\n    }\n}\n"],"names":["BaseLLMOutputParser","parseResultWithPrompt","generations","_prompt","callbacks","this","parseResult","_baseMessageToString","message","content","_baseMessageContentToString","JSON","stringify","invoke","input","options","_callWithConfig","async","text","runType","BaseOutputParser","parse","parseWithPrompt","_type","Error","self","location","origin","pathname","search","NoOpOutputParser","constructor","super","arguments","Object","defineProperty","enumerable","configurable","writable","value","lc_name","Promise","resolve","getFormatInstructions","_getLanguageModel","llmLike","_llmType","isBaseLanguageModel","isRunnable","bound","runnable","default","LLMChain","inputKeys","prompt","inputVariables","outputKeys","outputKey","fields","llm","llmKwargs","outputParser","getCallKeys","callKeys","_selectMemoryInputs","values","valuesForMemory","key","_getFinalOutput","promptValue","runManager","finalCompletion","getChild","call","config","_call","valuesForPrompt","valuesForLLM","formatPromptValue","generatePrompt","modelWithParser","pipe","response","predict","callbackManager","_chainType","deserialize","data","serialize","undefined","_getNumTokens","getNumTokens"],"sourceRoot":""}